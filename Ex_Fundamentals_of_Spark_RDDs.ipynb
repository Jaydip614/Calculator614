{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaydip614/Calculator614/blob/main/Ex_Fundamentals_of_Spark_RDDs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfbhu2HcUwuB"
      },
      "source": [
        "# Apache Spark Fundamentals: RDDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uux7_g7vUwuG"
      },
      "source": [
        "In this notebook we will work with the RDDs that are part of the Spark Core. The implementation of Spark Core is a **RDD (Resilient Distributed Dataset)** which is a collection of data distributed in different nodes of the cluster that are processed in parallel.\n",
        "\n",
        "We will use the PySpark API, but the concepts apply equally to all APIs (Scala, R, etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzlvQm2OUwuH"
      },
      "source": [
        "### Initializing Spark on Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9cWi_w6U-_a"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "Z6rkLRKeWX9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlE6ww6GWaqZ",
        "outputId": "f045f63f-a388-4723-c18b-244e3c8320ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "JKGWOaZXWfFC",
        "outputId": "ed8f15e7-773e-4aaa-a57a-29d732f1928d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a3ad4572290>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://27e5bf66d800:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xED4aFEnUwuI"
      },
      "outputs": [],
      "source": [
        "#!conda install -c conda-forge findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JYdfw_oUwuK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMaQ4ZAPUwuL"
      },
      "source": [
        "### Create the SparkSession and SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNzurql5UwuL"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName('PySpark_training')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q92NQjIEUwuM"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7MsQpeMUwuN"
      },
      "source": [
        "### Create an RDD from a collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly0XJUDDUwuO",
        "outputId": "a3f96703-2aae-4b7b-b153-2192742ef7e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "num = [1,2,3,4,5]\n",
        "\n",
        "num_rdd = sc.parallelize(num)\n",
        "num_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0NipnCPUwuP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q22dOlw4UwuQ"
      },
      "source": [
        "# Transformations\n",
        "* Transformations are lazy in nature and will not be executed until an Action is executed on them.\n",
        "* Let's try to understand the different transformations available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVYvcPOZUwuQ"
      },
      "source": [
        "### map\n",
        "* This will map your input to some output based on the function specified in the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvDa-CLsUwuR",
        "outputId": "5e9d2fc5-d8af-4826-8c24-4c431e2ae06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "double_rdd = num_rdd.map(lambda x : x * 2)\n",
        "double_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGAWeta2UwuR"
      },
      "source": [
        "### filter\n",
        "* To filter the data based on a certain condition. Let's try to find the even numbers of num_rdd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dGAHXV7UwuS",
        "outputId": "0aff1520-1b9c-4cf8-808a-d7cf630d5f07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "even_rdd = num_rdd.filter(lambda x : x % 2 == 0)\n",
        "even_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49x5-j3lUwuS"
      },
      "source": [
        "### flatMap\n",
        "* This function is very similar to map, but it can return multiple elements for each entry in the given RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4R2xTl-UwuT",
        "outputId": "c083d912-131f-4d4a-947f-6d7c0fcabdb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "flat_rdd = num_rdd.flatMap(lambda x : range(1,x))\n",
        "flat_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOo1BtdlUwuT"
      },
      "source": [
        "### distinct\n",
        "* This will return items other than an RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg1WEtT5UwuT",
        "outputId": "f59f7e79-021e-45b1-b777-6f4d181e772b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 12, 11]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "rdd1 = sc.parallelize([10, 11, 10, 11, 12, 11])\n",
        "dist_rdd = rdd1.distinct()\n",
        "dist_rdd.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfjG_QKfUwuU"
      },
      "source": [
        "### reduceByKey\n",
        "* This function reduces key value pairs based on keys and a given function within reduceByKey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhnuehmlUwuU",
        "outputId": "52aaacd4-6291-42be-d28d-a237a9024b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('b', 8)\n",
            "('c', 6)\n",
            "('a', 8)\n"
          ]
        }
      ],
      "source": [
        "pairs = [ (\"a\", 5), (\"b\", 7), (\"c\", 2), (\"a\", 3), (\"b\", 1), (\"c\", 4)]\n",
        "pair_rdd = sc.parallelize(pairs)\n",
        "\n",
        "output = pair_rdd.reduceByKey(lambda x, y : x + y)\n",
        "\n",
        "result = output.collect()\n",
        "print(*result, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEMcNuQoUwuV"
      },
      "source": [
        "### groupByKey\n",
        "* This function is another ByKey function that can operate on an RDD (key, value pair)  but this will only group the values based on the keys. In other words, this will only perform the first step of reduceByKey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OSU3bDOUwuV",
        "outputId": "33c81811-b64a-4eb0-9667-1803943b242b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', <pyspark.resultiterable.ResultIterable at 0x7ff1fbfeb070>),\n",
              " ('c', <pyspark.resultiterable.ResultIterable at 0x7ff1fbe85de0>),\n",
              " ('a', <pyspark.resultiterable.ResultIterable at 0x7ff1fbe85c00>)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "grp_out = pair_rdd.groupByKey()\n",
        "grp_out.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_BaRi-fUwuV"
      },
      "source": [
        "### sortByKey\n",
        "* This function will perform sorting on an RDD (key, value) pair based on the keys. By default, the sorting will be done in ascending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyBu7k7bUwuV",
        "outputId": "4af7ec75-9337-43fb-d1b2-2dfffd40b9aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('a', 5)\n",
            "('b', 3)\n",
            "('c', 2)\n",
            "('d', 7)\n"
          ]
        }
      ],
      "source": [
        "pairs = [ (\"a\", 5), (\"d\", 7), (\"c\", 2), (\"b\", 3)]\n",
        "raw_rdd = sc.parallelize(pairs)\n",
        "\n",
        "sortkey_rdd = raw_rdd.sortByKey()\n",
        "result = sortkey_rdd.collect()\n",
        "print(*result,sep='\\n')\n",
        "\n",
        "# Para clasificar en orden descendente, pase  “ascending=False”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8eTSvBGUwuW"
      },
      "source": [
        "### Sort by\n",
        "* sortBy is a more general function for sorting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N43D4X68UwuX",
        "outputId": "f1fc6413-fb5f-4c69-fa19-3ac8ac9d508e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('b', 3, 9)\n",
            "('a', 5, 10)\n",
            "('c', 2, 11)\n",
            "('d', 7, 12)\n"
          ]
        }
      ],
      "source": [
        "# Create RDD.\n",
        "pairs = [ (\"a\", 5, 10), (\"d\", 7, 12), (\"c\", 2, 11), (\"b\", 3, 9)]\n",
        "raw_rdd = sc.parallelize(pairs)\n",
        "\n",
        "# Let’s try to do the sorting based on the 3rd element of the tuple.\n",
        "sort_out = raw_rdd.sortBy(lambda x : x[2])\n",
        "result = sort_out.collect()\n",
        "print(*result, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tALDdXMQUwuX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQNR-UqXUwuY"
      },
      "source": [
        "# Actions\n",
        "\n",
        "* Actions are operations on RDD that are executed immediately. While transformations return another RDD, actions return native data structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwgmvn84UwuY"
      },
      "source": [
        "### count\n",
        "* This will count the number of items in the given RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwvVXVPoUwuY",
        "outputId": "c012643f-2633-44cf-ccc2-cc79dde97b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "num = sc.parallelize([1,2,3,4,2])\n",
        "num.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxkdp0NyUwuZ"
      },
      "source": [
        "### first\n",
        "* This will return the first element of the given RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TVn2hgUUwuZ",
        "outputId": "9c23cede-9999-4e92-a7a8-889c2dba20e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "num.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzR_7DoeUwuZ"
      },
      "source": [
        "### Collect\n",
        "* This will return all elements for the given RDD.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kry3FYKUwuZ",
        "outputId": "7901e7c0-6e4e-4b87-e290-bf02323d6b21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "num.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIEi5G6OUwua"
      },
      "source": [
        "**We should not use the collect operation while working with large data sets**. Because it will return all the data that is distributed between the different workers of the cluster to a controller. All the data will travel through the network from the worker to the driver and also the driver would need to store all the data. This will hamper the performance of your application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27i0z7QSUwua"
      },
      "source": [
        "### Take\n",
        "* This will return the number of items specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDJUojWNUwu2",
        "outputId": "992a3cd3-5574-4e79-bc82-39aeeaf15805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "num.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1 = sc.parallelize([1, 2, 3])\n",
        "rdd2 = sc.parallelize([3, 4, 5])\n",
        "union_rdd = rdd1.union(rdd2)\n",
        "print(\"Example for Union:\", union_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLFBbNdeisx9",
        "outputId": "66a035f1-b0cc-4ed8-a626-fd3385da2bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example for Union: [1, 2, 3, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intersection_rdd = rdd1.intersection(rdd2)\n",
        "print(\"Example for intersection:\", intersection_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FR-hL9tjwdi",
        "outputId": "589ebd36-faf8-4aef-c703-2c8b58abf4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example for intersection: [3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_rdd = union_rdd.distinct()\n",
        "print(\"Example for disctinct:\", distinct_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6bJhct-j2xh",
        "outputId": "8060ebe8-0b7c-4a32-aed5-6bd4f607ab73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example for disctinct: [4, 1, 5, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cartesian_rdd = rdd1.cartesian(rdd2)\n",
        "print(\"Example for cartesian:\", cartesian_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWwgowetkA0z",
        "outputId": "b2fc0aee-5a55-4902-c882-3c4af5623ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example for cartesian: [(1, 3), (1, 4), (1, 5), (2, 3), (3, 3), (2, 4), (2, 5), (3, 4), (3, 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_value = (0, 0)\n",
        "seq_op = lambda acc, value: (acc[0] + value, acc[1] + 1)\n",
        "comb_op = lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "average_result = num_rdd.aggregate(zero_value, seq_op, comb_op)\n",
        "print(\"Example agg:\", average_result[0] / average_result[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qOTJSlckHxX",
        "outputId": "2bbbaf27-7433-48da-a462-0e14979cbdcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example agg: 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: (date, temperature)\n",
        "temperature_data = [\n",
        "    (\"2022-01-01\", 25.0),\n",
        "    (\"2022-01-02\", 28.5),\n",
        "    (\"2022-02-01\", 22.0),\n",
        "    (\"2022-02-02\", 24.5),\n",
        "    (\"2022-03-01\", 18.0),\n",
        "    (\"2022-03-02\", 20.5),\n",
        "    # Add more data as needed\n",
        "]\n",
        "\n",
        "# Create an RDD from the sample data\n",
        "temperature_rdd = sc.parallelize(temperature_data)\n",
        "\n",
        "# Extract month and temperature as key-value pairs\n",
        "month_temperature_rdd = temperature_rdd.map(lambda x: (x[0][:7], (x[1], 1)))\n",
        "\n",
        "# Aggregate temperatures and counts by month\n",
        "sum_counts_by_month = month_temperature_rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "\n",
        "# Calculate average temperature for each month\n",
        "average_temperature_by_month = sum_counts_by_month.mapValues(lambda x: x[0] / x[1])\n",
        "\n",
        "# Collect and print the result\n",
        "result = average_temperature_by_month.collect()\n",
        "for month, average_temp in result:\n",
        "    print(f\"Month: {month}, Average Temperature: {average_temp:.2f}°C\")\n",
        "\n",
        "# Stop Spark\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojrWuT99k3rA",
        "outputId": "46099100-f020-4d3f-cf8f-b2e7e5f6b266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Month: 2022-03, Average Temperature: 19.25°C\n",
            "Month: 2022-01, Average Temperature: 26.75°C\n",
            "Month: 2022-02, Average Temperature: 23.25°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd = sc.textFile(\"Sample.txt\")\n",
        "\n",
        "# Perform transformations and actions\n",
        "word_counts = (text_rdd.flatMap(lambda line: line.split())\n",
        "                         .map(lambda word: (word, 1))\n",
        "                         .reduceByKey(lambda a, b: a + b)\n",
        "                         .sortByKey())\n",
        "\n",
        "# Display result\n",
        "print(\"Word Counts:\")\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "tc7yu70hn0Ak",
        "outputId": "11eeffe4-37e9-4356-fbcd-a19d1a99bcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-f18c35cff5cc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Perform transformations and actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m word_counts = (text_rdd.flatMap(lambda line: line.split())\n\u001b[1;32m      5\u001b[0m                          \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mtextFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m'Hello world!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \"\"\"\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m         return RDD(self._jsc.textFile(name, minPartitions), self,\n\u001b[1;32m    648\u001b[0m                    UTF8Deserializer(use_unicode))\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mdefaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         reduce tasks)\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}